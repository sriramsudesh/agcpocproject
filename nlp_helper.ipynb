{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
                }
            ],
            "source": "#### IBM Reserved Code.\n\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport string\nimport bisect\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom datetime import datetime, timedelta\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import LabelEncoder\nfrom wordsegment import load, segment\n# load() #Load the Segmentation model\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.parsing.preprocessing import STOPWORDS\n\nslang_hash = {'monies': 'money', 'cust': 'customer', 'cus': 'customer', 'custome': 'customer',\n              'fos': 'financial_ombudsman_service',\n              'afca': 'australian_financial_complaints_authority', 'gtr': 'guarantor', 'cad': 'card', 'kbonus': 'bonus',\n              'pch': 'primary_card_holder',\n              'rat': 'rate', 'didnt': \"did not\", 'feb': 'February', 'isn': 'is not'}\n\ncustom_bi_trigrams = {'break_free_package', 'financial_ombudsman_service', 'frequent_fly_points',\n                      'eligible_bonus_points',\n                      'business_select_package', 'platinum_card', 'frequent_flyer_black', 'bonus_point_offer',\n                      'frequent_flyer_credit',\n                      'reward_program_fee', 'residential_investment_property_loan', 'senior_personal_banker'}\n\nstop_words = stopwords.words(\"english\")\nstop_words = set(stop_words)\ncustom_stopwords = ['anz', 'account', 'customer', 'card', 'bank', 'complaint', 'january', 'february', 'march', 'april',\n                    'may', 'june', 'july', 'august',\n                    'september', 'october', 'november', 'december', 'customers']\n# custom_stopwords = ['anz','account','customer','bank','complaint']\nmy_stop_words = STOPWORDS.union(set(custom_stopwords))\n\n# Download the punkt tokenizer for sentence splitting\nimport nltk.data\n\nnltk.download('punkt')\n\n# Load the punkt tokenizer\ntokeniser = nltk.data.load('tokenizers/punkt/english.pickle')\n\n\ndef tokenizer(x):\n    return x\n\n\n# pip install --user gensim\nfrom gensim.models import word2vec\nimport logging\n\n\nclass EmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim = len(word2vec.itervalues().next())\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n\n\nclass TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        self.dim = len(word2vec.itervalues().next())\n\n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        # if a word was never seen - it must be at least as infrequent\n        # as any of the known words - so the default idf is the max of \n        # known idf's\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] * self.word2weight[w]\n                     for w in words if w in self.word2vec] or\n                    [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n\n\nclass DataFrameFeatureUnion(BaseEstimator, TransformerMixin):\n    \"\"\"Concats all the dataframe produced by a list of transformers.\n    Accepts a list of transformers as a parameter.\n    Returns the concatted dataframe with all the transformed dataframes.\"\"\"\n\n    def __init__(self, list_of_transformers):\n        self.list_of_transformers = list_of_transformers\n\n    def transform(self, X, **transformparamn):\n        concatted = pd.concat([transformer.transform(X)\n                               for transformer in\n                               self.fitted_transformers_], axis=1).copy()\n        return concatted\n\n    def fit(self, X, y=None, **fitparams):\n        self.fitted_transformers_ = []\n        for transformer in self.list_of_transformers:\n            fitted_trans = clone(transformer).fit(X, y=None, **fitparams)\n            self.fitted_transformers_.append(fitted_trans)\n        return self\n\n\n### Datetime transformers\n\nclass DatetimeTransfomer(BaseEstimator, TransformerMixin):\n    \"\"\"Converts a pandas column into the specified datetime format\n    and returns the dataframe.\n    Accepts a column and its format as parameters.\"\"\"\n\n    def __init__(self, column, datetime_format):\n        self.column = column\n        self.datetime_format = datetime_format\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        datetime_col = pd.to_datetime(X[self.column], format=self.datetime_format)\n        return pd.DataFrame(datetime_col, index=X.index, columns=[self.column])\n\n\nclass DatetimeDeltaTransfomer(BaseEstimator, TransformerMixin):\n    \"\"\"Calculates the datetime delta(in days) between two columns.\n    Accepts a pair a datetime column names as a parameter\n    and returns the delta column. Specify the defaut delta \n    if one of the values is null\"\"\"\n\n    def __init__(self, date_pair, default_diff=20000):\n        self.date_pair = date_pair\n        self.default_diff = default_diff\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        start = X[self.date_pair[0]]\n        end = X[self.date_pair[1]]\n        start_cond = start.dtype == np.dtype('datetime64[ns]')\n        end_cond = end.dtype == np.dtype('datetime64[ns]')\n        conditions = [\n            (pd.notnull(start) & pd.notnull(end)),\n            (pd.isnull(start) & pd.isnull(end))\n        ]\n        choices = [\n            abs((start - end) / timedelta(days=1)),\n            0\n        ]\n        if start_cond and end_cond:\n            date_col = np.select(conditions, choices, default=self.default_diff)\n            return pd.DataFrame(date_col, index=X.index,\n                                columns=['delta_{}_{}'.format(self.date_pair[0], self.date_pair[1])])\n\n\n### Numeric transformer\n\nclass NumericTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Converts a pandas column to numeric format.\n    Accepts a column name as a parameter and returns the transformed array\"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        col_num = pd.to_numeric(X[self.column])\n        return pd.DataFrame(col_num, index=X.index, columns=[self.column])\n\n\n### Text Cleaning\nclass TextCleaningTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Cleans the text column and returns the cleaned text\n    implemented cleaning methods:\n    1. remove punctuation through str.maketrans by turning on remove_punctuation\n       in fact this is replacing punctuation with space\n    2. remove blank elements by turning on remove_blank\n    3. remove tokens in a customized list of words by passing the list to remove_words\n    4. remove numbers\n    \"\"\"\n\n    def __init__(self, column, remove_number=True, start=None, end=None, remove_punctuation=True, remove_blank=True,\n                 remove_words=None,\n                 wordsegment=True, lem=True):\n        self.column = column\n        self.remove_number = remove_number\n        self.remove_punctuation = remove_punctuation\n        self.remove_blank = remove_blank\n        self.trantab = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n        self.remove_words = [] if remove_words == None else remove_words\n        # self.stem = stem\n        self.lem = lem\n        self.wordsegment = wordsegment\n\n    def _remove_number(self, row):\n        if self.remove_number:\n            return re.sub(r'[0-9]', ' ', row)\n        else:\n            return row\n\n    def _remove_blank(self, row):\n        if self.remove_blank:\n            return re.sub(r' +', ' ', row)\n        else:\n            return row\n\n    def _remove_words(self, row):\n        return ' '.join([token for token in row.split() if token not in self.remove_words])\n\n    def _remove_slang(self, row):\n        return ' '.join([slang_hash[token] if token in slang_hash else token for token in row.split()])\n\n    def _lem(self, row):\n        if self.lem:\n            lemmatizer = WordNetLemmatizer()\n            return ' '.join([lemmatizer.lemmatize(token, pos='v') for token in row.split()])\n        else:\n            return row\n\n    def _wordsegment(self, row):\n        load()\n        if self.wordsegment:\n            return ' '.join([segment(word) for token in row.split() for word in token])\n        else:\n            return row\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        X_column = X[self.column].str.translate(self.trantab) if self.remove_punctuation else X[self.column]\n        documents = X_column.astype('str').apply(self._remove_number).apply(self._remove_blank).apply(\n            self._remove_words).apply(self._remove_slang).apply(self._lem)\n        return pd.DataFrame(documents, index=X.index, columns=[self.column])\n\n\n### Text transformers\n\nclass TextTokenizerTransfomer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Tokenizes the bag column according to the separator \n    defined, clean the tokens, and returns the tokenized pandas series\n    implemented cleaning methods:\n    1. remove punctuation through str.maketrans by turning on remove_punctuation\n       in fact this is replacing punctuation with space\n    2. remove blank elements by turning on remove_blank\n    3. remove tokens in a customized list of words by passing the list to remove_words\n    4. slice tokens by index in the final cleaned token list, per row,\n       by passing start index and/or end index to start and end\n       e.g., start = 0 and end = 5 will do tokens[0:5]\n    \"\"\"\n\n    def __init__(self, column, separator=';', start=None, end=None):\n        self.column = column\n        self.separator = separator\n        self.start = start\n        self.end = end\n\n    def _splitter(self, row):\n        tokens = []\n        row = row.lower()\n        if self.separator in row:\n            tokens.extend(row.split(self.separator))\n        else:\n            tokens.append(row)\n        return tokens\n\n    def _remove_blank(self, tokens):\n        return [token for token in tokens if token != '']\n\n    def _slice(self, tokens):\n        return tokens[self.start:self.end]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        X_column = X[self.column]\n        documents = X_column.astype('str').apply(self._splitter).apply(self._remove_blank).apply(self._slice)\n        return pd.DataFrame(documents, index=X.index, columns=[self.column])\n\n\nclass BagVectorizerTransfomer(BaseEstimator, TransformerMixin):\n    \"\"\"Performs a bag-of-items encoding of the bag column\n    Accepts column name as a parameter and returns the encoding array\"\"\"\n\n    def __init__(self, column, remove_lowfreq=True, threshold_lowfreq=5):\n        self.column = column\n        self.vocab = None\n        self.uniq_tokens_remained = []\n        self.remove_lowfreq = remove_lowfreq\n        self.threshold_lowfreq = threshold_lowfreq\n\n    def fit(self, X, y=None, **fit_params):\n        tokens = np.concatenate(X[self.column].values)\n        uniq_tokens = list(set(tokens))\n        uniq_tokens.sort()  # to ensure the order is always the same\n\n        if self.remove_lowfreq:\n            vocab_freq = Counter(tokens)\n            vocab_remove = [k for k, v in vocab_freq.items() if v < self.threshold_lowfreq] + ['']\n        else:\n            vocab_remove = []\n\n        uniq_tokens_remained = [token for token in uniq_tokens if token not in vocab_remove]\n        vocab = {k: v for v, k in enumerate(uniq_tokens_remained)}\n        self.vocab = vocab\n        self.uniq_tokens_remained = uniq_tokens_remained\n        return self\n\n    def get_feature_names(self):\n        # return [self.column + '_{}'.format(token) for token, in self.vocab.keys()]\n        return [self.column + '_{}'.format(token) for token in self.uniq_tokens_remained]\n\n    def transform(self, X, **transform_params):\n        documents = X[self.column]\n        arr = np.zeros((X.index.size, len(self.vocab)))\n        for index, doc in enumerate(documents):\n            tokens_index = [self.vocab[token] for token in doc if token in self.vocab]\n            for tok_idx in tokens_index:\n                arr[index, tok_idx] += 1\n        return pd.DataFrame(arr, index=X.index, columns=self.get_feature_names())\n\n\n### Word2Vec Transformers\n\n#### Preprocessing\n\ndef sentence_to_words(sentence, remove_stopwords=True):\n    \"\"\"Function to convert a sentence to a sequence of words,\n    optionally removing stop words.  Returns a list of words.\"\"\"\n\n    # Remove non-letters\n    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n\n    # Convert words to lower case and split them\n    words = sentence.lower().split()\n\n    # Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = my_stop_words\n        words = [w for w in words if not w in stops]\n\n    return words\n\n\ndef document_to_sentences(document, remove_stopwords=True):\n    \"\"\" Function to split a document into parsed sentences. \n    Returns a list of sentences, where each sentence is a \n    list of words \"\"\"\n\n    # Use NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokeniser.tokenize(document.strip())\n\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call sentence_to_words to get a list of words\n            sentences.extend(sentence_to_words(raw_sentence, remove_stopwords))\n\n    return sentences\n\n\nclass TokenizerTransfomer(BaseEstimator, TransformerMixin):\n    \"\"\"Tokenizes a column and returns the tokenized pandas series\"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        documents = X[self.column].astype('str').apply(document_to_sentences)\n        return pd.DataFrame(documents, index=X.index, columns=[self.column])\n\n\nclass DataFrameTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Makes Custom modifications to Compliance Dataset\"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        text_clean = TextCleaningTransformer(column=self.column, remove_words=my_stop_words)\n        tokenise = TokenizerTransfomer(column=self.column)\n        df = text_clean.transform(X=X)\n        final_d = tokenise.transform(X=df)\n        final_d.columns = ['features']\n        X['features'] = final_d['features']\n        # final_df = X[['Breach','features']]\n        X = X.drop(self.column, axis=1)\n        # print(pd.DataFrame(X,columns =['features']))\n        return pd.DataFrame(X, columns=['features'])\n        # return pd.DataFrame(X,columns =['Breach','features'])\n\n\nclass ColumnExtractorTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Makes Custom modifications to Compliance Dataset\"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        return X[self.column]\n\n\nclass Word2VecTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Converts a tokenized pandas series into an encoding array\n    of the specified size.\"\"\"\n\n    def __init__(self, column, size=10):\n        self.size = size\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        model = word2vec.Word2Vec(X[self.column], size=self.size, min_count=0)\n        model.init_sims(replace=True)\n        self.model = model\n        return self\n\n    def transform(self, X, **transform_params):\n        arr = np.zeros((X.shape[0], self.size))\n        for i, doc in enumerate(X[self.column]):\n            row = np.zeros(self.size)\n            for token in doc:\n                if token in self.model.wv:\n                    row += self.model.wv[token]\n            if len(doc) != 0:\n                arr[i] = row / len(doc)\n            else:\n                arr[i] = row\n        col_names = [self.column + '_dim_{}'.format(i) for i in range(self.size)]\n        return pd.DataFrame(arr, index=X.index, columns=col_names)\n\n\nclass DropFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"Drops User defined features from the pipeline\"\"\"\n\n    def __init__(self, column_list):\n        self.column_list = column_list\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        # print(X)\n        return X.drop(self.column_list, axis=1)\n\n\nclass CustomLabelEncoder(BaseEstimator, TransformerMixin):\n    \"\"\" This is a custom label encoder which wraps sklearn's\n        LabelEncoder and makes it compatible with its pipeline.\n        It takes in a column(list or pandas series) as its input.\n        Returns the transformed array\n        We keep the null values as is but take care of any \n        unknown classes not present in the training set apprearing \n        in the test set with '<unknown>'\n        Note: Beware that no value in your dataset has a string\n        called '<unkown>' , if there exists, then in the\n        code modify it into anything unique\"\"\"\n\n    def __init__(self, column):\n        self.column = column\n        self.encoder = LabelEncoder()\n\n    def fit(self, X, y=None, **fit_params):\n        original_col_without_nans = X[self.column][pd.notnull(X[self.column])].astype('str')\n        self.encoder.fit(original_col_without_nans)\n        le_classes = self.encoder.classes_.tolist()\n        bisect.insort_left(le_classes, '<unknown>')\n        self.encoder.classes_ = le_classes\n        return self\n\n    # unique value function\n\n    def transform(self, X, **transform_params):\n        original_col = X[self.column].get_values()\n        original_col_without_nans = X[self.column][pd.notnull(X[self.column])]\n        label_col = original_col_without_nans.map(\n            lambda s: '<unknown>' if s not in self.encoder.classes_ else s).astype('str')\n        transformed_arr = self.encoder.transform(label_col)\n        original_col[pd.notnull(original_col)] = transformed_arr\n        return pd.DataFrame(original_col, index=X.index, columns=[self.column]).astype('float')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                },
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import nltk\nnltk.download('stopwords')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: wordsegment in /home/dsxuser/.local/lib/python3.6/site-packages (1.3.1)\nCollecting gensim\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.2MB 6.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.12.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.5.2)\nCollecting smart-open>=1.8.1 (from gensim)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/9c/2ee26604648f92251f26a00a79fd164079163693f53792a3ba99f6152349/smart_open-2.2.1.tar.gz (122kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 53.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.19.2)\nRequirement already satisfied: requests in /opt/conda/envs/Python36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\nRequirement already satisfied: boto3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.9.82)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\nRequirement already satisfied: botocore<1.13.0,>=1.12.82 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.82)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.3)\nRequirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.1.13)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.82->boto3->smart-open>=1.8.1->gensim) (0.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.82->boto3->smart-open>=1.8.1->gensim) (2.7.5)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/6a/25/34/a5afefe4e3cad127e65c9bd1b6440c1916feb0bf2f744001e2\nSuccessfully built smart-open\nInstalling collected packages: smart-open, gensim\nSuccessfully installed gensim-3.8.3 smart-open-2.2.1\n"
                }
            ],
            "source": "!pip install --user wordsegment gensim"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}